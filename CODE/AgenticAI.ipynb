{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Installing Required Packages**"
      ],
      "metadata": {
        "id": "sjWx2CwzFb3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-genai google-generativeai langchain langchain-community"
      ],
      "metadata": {
        "id": "Y45CokbkqH49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "FzZGa25fekkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-search-results"
      ],
      "metadata": {
        "id": "efjG9jSzs-mD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_classic langchain-community"
      ],
      "metadata": {
        "id": "S9ejon8-Lfcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading API keys from Colab Secrets**"
      ],
      "metadata": {
        "id": "SIZI82X9FryY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "gkey = userdata.get('GAPI')\n",
        "skey = userdata.get('SERP_KEY')"
      ],
      "metadata": {
        "id": "p3jAyzRnAi3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Simple Agent that call tools based on the input**\n",
        "Here the tools are mocked with functions\n"
      ],
      "metadata": {
        "id": "4liB8tjZF9YQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_classic.tools import Tool\n",
        "from langchain_classic.agents import initialize_agent, AgentType, load_tools, AgentExecutor\n",
        "\n",
        "def calc_tool(expr: str) -> str:\n",
        "        return str(eval(expr))\n",
        "\n",
        "def search_tool(q: str) -> str:\n",
        "    return \"SEARCH_RESULT: product X released in 2010\"\n",
        "\n",
        "t1=Tool(name=\"search\", func=search_tool, description=\"Search for product info and return results.\")\n",
        "t2=Tool(name=\"calc\", func=calc_tool, description=\"Evaluate arithmetic expressions safely.\")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",temperature=0,\n",
        "    google_api_key=gkey)\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools=[t1,t2],llm=llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,verbose=True)\n",
        "\n",
        "print(agent.run('Find the year product X released (search), then compute years since release (calc).'))\n"
      ],
      "metadata": {
        "id": "sxTF1vIUqEU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Simple Agent that call tools based on the input**\n",
        "Here the tools are actual api endpoinds serpapi and llm-math\n"
      ],
      "metadata": {
        "id": "icOl9HpUGZYs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r64xxht5Ov_E"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.agents import initialize_agent, load_tools\n",
        "\n",
        "# Initialize Gemini LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",temperature=0,\n",
        "    google_api_key=gkey\n",
        ")\n",
        "tools = load_tools(\n",
        "    [\"serpapi\", \"llm-math\"],llm=llm,\n",
        "    serpapi_api_key=skey\n",
        ")\n",
        "agent = initialize_agent(\n",
        "    tools=tools,llm=llm,agent=\"zero-shot-react-description\",verbose=True\n",
        ")\n",
        "\n",
        "# Run the query\n",
        "result = agent.run(\"What's the weather in Paris and square root of 123?\")\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Agentic RAG : An Agent Based RAG Application**\n"
      ],
      "metadata": {
        "id": "GqGzkje2Gq7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain imports"
      ],
      "metadata": {
        "id": "LZSGCXxeHTvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import ast\n",
        "import operator as op\n",
        "from typing import List\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_classic.tools import Tool\n",
        "from langchain_classic.agents import initialize_agent, AgentType, load_tools\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_classic.chains import RetrievalQA\n",
        "from langchain_classic.memory import ConversationBufferMemory\n",
        "from langchain_classic import LLMChain, PromptTemplate\n"
      ],
      "metadata": {
        "id": "d-a4_H7gWoZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Document loading & FAISS index\n"
      ],
      "metadata": {
        "id": "equsEnb2HYAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_tool(expr: str) -> str:\n",
        "  res = eval(expr)\n",
        "  return str(res)\n",
        "\n",
        "\n",
        "def load_documents(doc_folder: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Simple loader: reads .txt files from directory and returns list[str].\n",
        "    Replace/extend this to read PDFs, DOCX, or use langchain.document_loaders.\n",
        "    \"\"\"\n",
        "    texts = []\n",
        "    p = Path(doc_folder)\n",
        "    for f in p.glob(\"**/*.txt\"):\n",
        "        texts.append(f.read_text(encoding=\"utf-8\"))\n",
        "    return texts\n",
        "\n",
        "def build_faiss_from_texts(texts: List[str], persist_dir: str = \"faiss_index\"):\n",
        "    \"\"\"\n",
        "    - Splits large texts to chunks\n",
        "    - Builds embeddings using HuggingFaceEmbeddings (all-MiniLM)\n",
        "    - Creates FAISS index (persist optional)\n",
        "    \"\"\"\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
        "    docs = []\n",
        "    for t in texts:\n",
        "        docs.extend(splitter.split_text(t))\n",
        "\n",
        "    embed = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    faiss_db = FAISS.from_texts(docs, embed)\n",
        "\n",
        "    # optional persist\n",
        "    try:\n",
        "        faiss_db.save_local(persist_dir)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return faiss_db\n",
        "\n",
        "# -----------------------\n",
        "# Retriever tool wrapper\n",
        "# -----------------------\n",
        "def make_retrieval_tool(faiss_db, llm):\n",
        "    \"\"\"\n",
        "    Wrap retrieval + LLM as a tool: user gives a query, tool returns grounded summary\n",
        "    \"\"\"\n",
        "    retriever = faiss_db.as_retriever(search_kwargs={\"k\": 4})\n",
        "    qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"map_reduce\", retriever=retriever)\n",
        "\n",
        "    def retrieval_tool(query: str) -> str:\n",
        "        try:\n",
        "            return qa_chain.run(query)\n",
        "        except Exception as e:\n",
        "            return f\"ERROR during retrieval: {e}\"\n",
        "\n",
        "    return retrieval_tool"
      ],
      "metadata": {
        "id": "sxpGKSoWXj9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mock / real search tool (here we use SerpAPI wrapper via load_tools)"
      ],
      "metadata": {
        "id": "j50zCZbEHnyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_search_tool(llm, serpapi_key: str):\n",
        "    \"\"\"\n",
        "    Uses LangChain's built-in SerpAPI tool via load_tools for web search.\n",
        "    If serpapi key not present, returns a simple mock function.\n",
        "    \"\"\"\n",
        "    if serpapi_key:\n",
        "        tools = load_tools([\"serpapi\"], llm=llm, serpapi_api_key=serpapi_key)\n",
        "        # load_tools returns a list; find the serpapi tool\n",
        "        for t in tools:\n",
        "            if getattr(t, \"name\", \"\").lower().startswith(\"serpapi\"):\n",
        "                return t.func\n",
        "    # fallback mock\n",
        "    def mock_search(q: str) -> str:\n",
        "        return f\"MOCK_SEARCH_RESULT for: {q}\"\n",
        "    return mock_search\n"
      ],
      "metadata": {
        "id": "mUhs3w_RXsmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main assembly of agent\n"
      ],
      "metadata": {
        "id": "gxo2rrt5HyAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def build_agent(\n",
        "    doc_folder: str,\n",
        "    google_api_key: str,\n",
        "    serpapi_key: str = None\n",
        "):\n",
        "    # 1) LLM (Gemini)\n",
        "    llm = ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-2.0-flash\",\n",
        "        temperature=0,\n",
        "        google_api_key=google_api_key\n",
        "    )\n",
        "\n",
        "    # 2) Build or load FAISS from local docs\n",
        "    texts = load_documents(doc_folder)\n",
        "    if not texts:\n",
        "        print(\"Warning: no local docs found. FAISS will be empty (use small demo text).\")\n",
        "        texts = [\"Product X launched in 2010. Sales grew from 1.2M to 2.0M in 2015-2020.\"]\n",
        "    faiss_db = build_faiss_from_texts(texts)\n",
        "\n",
        "    # 3) Create retrieval tool\n",
        "    retrieval_tool_fn = make_retrieval_tool(faiss_db, llm)\n",
        "    retrieval_tool = Tool(\n",
        "        name=\"doc_retriever\",\n",
        "        func=retrieval_tool_fn,\n",
        "        description=\"Use this to retrieve and summarize relevant passages from local documents. Input: a natural language question.\"\n",
        "    )\n",
        "\n",
        "    # 4) Search tool (SerpAPI or mock)\n",
        "    search_fn = make_search_tool(llm, serpapi_key)\n",
        "    search_tool = Tool(\n",
        "        name=\"web_search\",\n",
        "        func=search_fn,\n",
        "        description=\"Search the web and return short factual snippets about the query.\"\n",
        "    )\n",
        "\n",
        "    # 5) Calculator\n",
        "    calc_tool_obj = Tool(\n",
        "        name=\"calculator\",\n",
        "        func=calc_tool,\n",
        "        description=\"Safely evaluate arithmetic expressions.\"\n",
        "    )\n",
        "\n",
        "    # 6) Memory (short-term conversation buffer)\n",
        "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=False)\n",
        "\n",
        "    # 7) Initialize agent with tools\n",
        "    tools = [retrieval_tool, search_tool, calc_tool_obj]\n",
        "\n",
        "    agent = initialize_agent(\n",
        "        tools=tools,\n",
        "        llm=llm,\n",
        "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "        verbose=True,\n",
        "        memory=memory\n",
        "    )\n",
        "    return agent, faiss_db"
      ],
      "metadata": {
        "id": "rtNKHBDFX9Y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example usage"
      ],
      "metadata": {
        "id": "ESWw4pz9H-BS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # point to folder with .txt docs (or leave empty to trigger demo text)\n",
        "    DOC_FOLDER = \"/content/data_store\"\n",
        "\n",
        "    agent, db = build_agent(DOC_FOLDER, google_api_key=gkey, serpapi_key=skey)\n",
        "    # Complex multi-step prompt example:\n",
        "    prompt = (\n",
        "    \"Task: Prepare a concise analytical summary based on the Sherlock Holmes corpus provided. \"\n",
        "    \"1) Use doc_retriever to identify the title and collection in which the story 'A Scandal in Bohemia' appears. \"\n",
        "    \"2) Retrieve key character information about Sherlock Holmes and Irene Adler from the documents. \"\n",
        "    \"3) Extract one example of Holmes’s reasoning or deduction described in the text. \"\n",
        "    \"4) Produce a 4-bullet synthesis summarizing the narrative context, main characters, \"\n",
        "    \"Holmes’s analytical style, and the significance of the story within the Sherlock Holmes series.\"\n",
        "    )\n",
        "\n",
        "    print(\"\\n----- RUNNING AGENT -----\\n\")\n",
        "    output = agent.invoke(prompt)\n",
        "    print(\"\\n----- AGENT OUTPUT -----\\n\")\n",
        "    print(output)\n"
      ],
      "metadata": {
        "id": "i9aZsjP9CBu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Agentic CHAT BOT : An Agent Based Converational RAG Application**"
      ],
      "metadata": {
        "id": "ej0IK7pdJNgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import ast\n",
        "import operator as op\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_classic.agents import initialize_agent, AgentType, load_tools\n",
        "from langchain_classic.tools import Tool\n",
        "from langchain_classic.chains import RetrievalQA\n",
        "\n",
        "\n",
        "\n",
        "def calc_tool_fn(expr: str) -> str:\n",
        "    try:\n",
        "        return str(eval(expr))\n",
        "    except Exception as e:\n",
        "        return f\"CALC_ERROR: {e}\"\n",
        "\n",
        "# ---------------\n",
        "# Document loading & FAISS\n",
        "# ---------------\n",
        "def load_txts(folder: str) -> List[str]:\n",
        "    p = Path(folder)\n",
        "    texts = []\n",
        "    if not p.exists():\n",
        "        return texts\n",
        "    for f in p.glob(\"**/*.txt\"):\n",
        "        try:\n",
        "            texts.append(f.read_text(encoding=\"utf-8\"))\n",
        "        except Exception:\n",
        "            pass\n",
        "    return texts\n",
        "\n",
        "def build_faiss(texts: List[str], persist_dir: str = \"faiss_agentic\"):\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=120)\n",
        "    chunks = []\n",
        "    for t in texts:\n",
        "        chunks.extend(splitter.split_text(t))\n",
        "    embed = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    db = FAISS.from_texts(chunks, embed)\n",
        "    try:\n",
        "        db.save_local(persist_dir)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return db\n",
        "\n",
        "# ---------------\n",
        "# Build Gemini Agent with RAG + Calculator Tool\n",
        "# ---------------\n",
        "def build_agent(faiss_db, llm):\n",
        "    retriever = faiss_db.as_retriever(search_kwargs={\"k\": 4})\n",
        "    qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"map_reduce\", retriever=retriever)\n",
        "\n",
        "    tools = [\n",
        "        Tool(\n",
        "            name=\"retrieval\",\n",
        "            func=lambda q: qa_chain.invoke({\"query\": q}),\n",
        "            description=\"Useful for answering questions from local documents.\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"calculator\",\n",
        "            func=calc_tool_fn,\n",
        "            description=\"Useful for evaluating basic arithmetic expressions like '123 + 456'.\"\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    agent = initialize_agent(\n",
        "        tools=tools,\n",
        "        llm=llm,\n",
        "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "        verbose=True\n",
        "    )\n",
        "    return agent\n",
        "\n",
        "# ---------------\n",
        "# Main\n",
        "# ---------------\n",
        "def main():\n",
        "\n",
        "    docs = load_txts(\"/content/data_store\")\n",
        "    if not docs:\n",
        "        print(\"No docs found in ./docs — using default text\")\n",
        "    print(\"Building FAISS index...\")\n",
        "    db = build_faiss(docs)\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0, google_api_key=gkey)\n",
        "    agent = build_agent(db, llm)\n",
        "\n",
        "\n",
        "    while True:\n",
        "        query = input(\"You: \")\n",
        "        if query.lower() in ['exit', 'quit']:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        answer = agent.invoke({\"input\": query})\n",
        "        print(f\"Bot: {answer}\\n\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "nAAYuDYWLcqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LangGraph**"
      ],
      "metadata": {
        "id": "etKVPTs9JmUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Simple LLM Wrapper"
      ],
      "metadata": {
        "id": "lDiL9DRmJztd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# minimal_langgraph_gemini.py\n",
        "import os\n",
        "from pydantic import BaseModel, Field\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# -------- State --------\n",
        "class State(BaseModel):\n",
        "    input: str = Field(...)\n",
        "    output: str | None = None\n",
        "\n",
        "# -------- Gemini LLM --------\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0,\n",
        "    google_api_key=gkey\n",
        ")\n",
        "\n",
        "# -------- Node --------\n",
        "def respond(state: State) -> State:\n",
        "    raw = llm.invoke(state.input)\n",
        "    # minimal normalization without helpers\n",
        "    if hasattr(raw, \"content\"):\n",
        "        text = raw.content\n",
        "    else:\n",
        "        text = str(raw)\n",
        "    return State(input=state.input, output=text)\n",
        "\n",
        "# -------- Build Graph --------\n",
        "graph = StateGraph(State)\n",
        "graph.add_node(\"respond\", respond)\n",
        "graph.set_entry_point(\"respond\")\n",
        "graph.add_edge(\"respond\", END)\n",
        "app = graph.compile()\n",
        "\n",
        "# -------- Run --------\n",
        "result = app.invoke({\"input\": \"Explain LangGraph in one line\"})\n",
        "print(result['output'])\n"
      ],
      "metadata": {
        "id": "WXCj1T2j7UQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Mock Question Answering"
      ],
      "metadata": {
        "id": "R2ZCZsTXJ96O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "class State(BaseModel):\n",
        "    query: str = Field(...)\n",
        "    action: str | None = None\n",
        "    docs: list | None = None\n",
        "    answer: str | None = None\n",
        "\n",
        "# mock db & llm as above\n",
        "class MockDB:\n",
        "    def similarity_search(self, q):\n",
        "        return [type(\"D\", (), {\"page_content\": \"Product X released in 2010.\"})()]\n",
        "\n",
        "class MockLLM:\n",
        "    def invoke(self, prompt):\n",
        "        return f\"ANSWER_FROM_LLM: {prompt[:80]}...\"\n",
        "\n",
        "db = MockDB()\n",
        "llm = MockLLM()\n",
        "\n",
        "def planner(state: State):\n",
        "    q = state.query\n",
        "    state.action = \"retrieve\" if (\"year\" in q or \"released\" in q) else \"answer_direct\"\n",
        "    return state\n",
        "\n",
        "def retrieve(state: State):\n",
        "    state.docs = db.similarity_search(state.query)\n",
        "    return state\n",
        "\n",
        "def answer(state: State):\n",
        "    context = \"\\n\".join([d.page_content for d in (state.docs or [])])\n",
        "    prompt = f\"Context:\\n{context}\\n\\nQuestion:\\n{state.query}\"\n",
        "    raw = llm.invoke(prompt)\n",
        "    text = getattr(raw, \"content\", raw)\n",
        "    state.answer = str(text)\n",
        "    return state\n",
        "\n",
        "graph = StateGraph(State)\n",
        "graph.add_node(\"planner\", planner)\n",
        "graph.add_node(\"retriever\", retrieve)\n",
        "graph.add_node(\"answer\", answer)\n",
        "\n",
        "graph.set_entry_point(\"planner\")\n",
        "graph.add_conditional_edges(\"planner\",\n",
        "    lambda s: s.action,\n",
        "    {\"retrieve\": \"retriever\", \"answer_direct\": \"answer\"}\n",
        ")\n",
        "graph.add_edge(\"retriever\", \"answer\")\n",
        "graph.add_edge(\"answer\", END)\n",
        "\n",
        "app = graph.compile()\n",
        "\n",
        "# Must pass 'query' when invoking because Pydantic requires it\n",
        "result = app.invoke({\"query\": \"When was Product X released?\"})\n",
        "\n",
        "print(\"answer:\", result['answer'])\n"
      ],
      "metadata": {
        "id": "H-uqFMvb1BVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Multi Agent Systems**"
      ],
      "metadata": {
        "id": "TvTQOYanfADf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MASState(BaseModel):\n",
        "    query: str\n",
        "    docs: list | None = None\n",
        "    draft: str | None = None\n",
        "    final: str | None = None\n",
        "    action: str | None = None\n",
        "\n",
        "# --- Planner Agent ---\n",
        "def planner(state):\n",
        "    if \"year\" in state.query or \"released\" in state.query:\n",
        "        state.action = \"retrieve\"\n",
        "    else:\n",
        "        state.action = \"reason_direct\"\n",
        "    return state\n",
        "\n",
        "# --- Retriever Agent ---\n",
        "def retriever_agent(state):\n",
        "    state.docs = db.similarity_search(state.query)\n",
        "    return state\n",
        "\n",
        "# --- Reasoner Agent (Gemini) ---\n",
        "def reasoner_agent(state):\n",
        "    ctx = \"\\n\".join(d.page_content for d in (state.docs or []))\n",
        "    prompt = f\"Context: {ctx}\\nQuestion: {state.query}\\nAnswer:\"\n",
        "    raw = llm.invoke(prompt)\n",
        "    state.draft = str(getattr(raw, \"content\", raw))\n",
        "    return state\n",
        "\n",
        "# --- Critic Agent ---\n",
        "def critic_agent(state):\n",
        "    critique = llm.invoke(\n",
        "        f\"Evaluate this answer:\\n{state.draft}\\nProvide corrections only.\"\n",
        "    )\n",
        "    state.final = str(getattr(critique, \"content\", critique))\n",
        "    return state\n",
        "\n",
        "# --- Build Graph ---\n",
        "graph = StateGraph(MASState)\n",
        "graph.add_node(\"planner\", planner)\n",
        "graph.add_node(\"retriever\", retriever_agent)\n",
        "graph.add_node(\"reasoner\", reasoner_agent)\n",
        "graph.add_node(\"critic\", critic_agent)\n",
        "\n",
        "graph.set_entry_point(\"planner\")\n",
        "graph.add_conditional_edges(\"planner\",\n",
        "    lambda s: s.action,\n",
        "    {\"retrieve\": \"retriever\", \"reason_direct\": \"reasoner\"}\n",
        ")\n",
        "graph.add_edge(\"retriever\", \"reasoner\")\n",
        "graph.add_edge(\"reasoner\", \"critic\")\n",
        "graph.add_edge(\"critic\", END)\n",
        "\n",
        "app = graph.compile()"
      ],
      "metadata": {
        "id": "zEIXaBa9e6gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = app.invoke({\"query\": \"When was Product X released?\"})\n",
        "\n",
        "print(\"Draft answer:\", result['draft'])\n",
        "print(\"Final corrected answer:\", result['final'])"
      ],
      "metadata": {
        "id": "1kF_YukYj986"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kxCcP0FDkK6s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}