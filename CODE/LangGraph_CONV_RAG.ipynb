{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Installing Essential Packages**"
      ],
      "metadata": {
        "id": "6v6HTvVvKdFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-genai google-generativeai langchain langchain-community"
      ],
      "metadata": {
        "id": "Y45CokbkqH49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "FzZGa25fekkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-search-results"
      ],
      "metadata": {
        "id": "efjG9jSzs-mD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_classic langchain-community"
      ],
      "metadata": {
        "id": "KiHSkKZBQOk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importing LangChain/LangGraph Tools**"
      ],
      "metadata": {
        "id": "xj35Cb1SKlSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# langgraph_rag_gemini.py\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List, Any\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# LangGraph\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# LLM (Gemini)\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Text splitting / embeddings / FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# Retrieval chain\n",
        "from langchain_classic.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "0CXIzcHiPF2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "key=userdata.get('GAPI')"
      ],
      "metadata": {
        "id": "qMz_E_FjdNpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2fdc9BFO1dY"
      },
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Simple document loader + FAISS builder\n",
        "# -------------------------\n",
        "def load_txts(folder: str) -> List[str]:\n",
        "    p = Path(folder)\n",
        "    texts = []\n",
        "    if not p.exists():\n",
        "        return texts\n",
        "    for f in p.glob(\"**/*.txt\"):\n",
        "        try:\n",
        "            texts.append(f.read_text(encoding=\"utf-8\"))\n",
        "        except Exception:\n",
        "            pass\n",
        "    return texts\n",
        "\n",
        "def build_faiss_from_texts(texts: List[str], persist_dir: str = \"faiss_index\"):\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=120)\n",
        "    chunks = []\n",
        "    for t in texts:\n",
        "        chunks.extend(splitter.split_text(t))\n",
        "\n",
        "    embed = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    db = FAISS.from_texts(chunks, embed)\n",
        "    try:\n",
        "        db.save_local(persist_dir)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return db\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Pydantic State model for LangGraph\n",
        "# -------------------------\n",
        "class RAGState(BaseModel):\n",
        "    query: str = Field(...)\n",
        "    action: str | None = None\n",
        "    docs: List[Any] | None = None     # store Document-like objects\n",
        "    answer: str | None = None\n",
        "\n",
        "# -------------------------\n",
        "# Setup LLM and FAISS (require GOOGLE_API_KEY)\n",
        "# -------------------------\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0, google_api_key=key)\n",
        "\n",
        "# Build FAISS index from ./docs (or seed demo)\n",
        "texts = load_txts(\"/content/data_store\")\n",
        "\n",
        "faiss_db = build_faiss_from_texts(texts)\n",
        "\n",
        "# RetrievalQA chain (Gemini used inside RetrievalQA for synthesis)\n",
        "retriever = faiss_db.as_retriever(search_kwargs={\"k\": 4})\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"map_reduce\", retriever=retriever)\n",
        "\n",
        "# -------------------------\n",
        "# Graph node functions\n",
        "# -------------------------\n",
        "def planner(state: RAGState) -> RAGState:\n",
        "    q = state.query.lower()\n",
        "    # simple heuristic planner\n",
        "    if any(tok in q for tok in (\"year\", \"released\", \"release\", \"when was\")):\n",
        "        state.action = \"retrieve\"\n",
        "    else:\n",
        "        state.action = \"answer_direct\"\n",
        "    return state\n",
        "\n",
        "def retriever_node(state: RAGState) -> RAGState:\n",
        "    # use FAISS similarity search to get docs (documents have .page_content)\n",
        "    docs = faiss_db.similarity_search(state.query, k=4)\n",
        "    state.docs = docs\n",
        "    return state\n",
        "\n",
        "def answer_node(state: RAGState) -> RAGState:\n",
        "    # build a short context and ask the LLM to answer grounded on it\n",
        "    docs = state.docs or []\n",
        "    context = \"\\n\\n\".join(getattr(d, \"page_content\", str(d)) for d in docs)\n",
        "    prompt = f\"Context:\\n{context}\\n\\nQuestion:\\n{state.query}\\n\\nAnswer concisely:\"\n",
        "    # call the qa_chain if available (it will use retriever internally) OR call llm with context\n",
        "    # Prefer qa_chain.invoke to get the RetrievalQA behavior if supported\n",
        "    if hasattr(qa_chain, \"invoke\"):\n",
        "        raw = qa_chain.invoke({\"query\": state.query})\n",
        "        # qa_chain.invoke may return dict or string; normalize in one line:\n",
        "        text = str(getattr(raw, \"content\", raw))\n",
        "    else:\n",
        "        raw = llm.invoke(prompt)\n",
        "        text = str(getattr(raw, \"content\", raw))\n",
        "    state.answer = text\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "jborVmG1R6XX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Build LangGraph\n",
        "# -------------------------\n",
        "graph = StateGraph(RAGState)\n",
        "graph.add_node(\"planner\", planner)\n",
        "graph.add_node(\"retriever\", retriever_node)\n",
        "graph.add_node(\"answer\", answer_node)\n",
        "\n",
        "graph.set_entry_point(\"planner\")\n",
        "# conditional edges from planner based on planner.action\n",
        "graph.add_conditional_edges(\"planner\", lambda s: s.action, {\"retrieve\": \"retriever\", \"answer_direct\": \"answer\"})\n",
        "graph.add_edge(\"retriever\", \"answer\")\n",
        "graph.add_edge(\"answer\", END)\n",
        "\n",
        "app = graph.compile()\n",
        "\n",
        "# -------------------------\n",
        "# Run example\n",
        "# -------------------------\n",
        "import ast\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # interactive test\n",
        "    while True:\n",
        "        q = input(\"\\nAsk (or 'exit'): \").strip()\n",
        "        if q.lower() in (\"exit\", \"quit\"):\n",
        "            break\n",
        "        out = app.invoke({\"query\": q})\n",
        "        print(\"\\n=>\", ast.literal_eval(out['answer'])['result'])\n"
      ],
      "metadata": {
        "id": "AZl45-8VR-3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GauJLg4mTGdv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}