{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Installing Essential Packages**"
      ],
      "metadata": {
        "id": "AaDj04g3i5_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-genai"
      ],
      "metadata": {
        "id": "VSma8v4CroLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "Pxg8FqdWyGGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain langchain-core langchain-community langchain-text-splitters\n"
      ],
      "metadata": {
        "id": "s9bnLEavyk6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers  # required for HuggingFace embeddings\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "-7_WWanU09AQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "id": "zTVxtGpGzEl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "gkey=userdata.get('GAPI')"
      ],
      "metadata": {
        "id": "sGR5GWSQMZB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PDF Loader**"
      ],
      "metadata": {
        "id": "2LcURj2EjGc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"/content/LoveStories.pdf\")\n",
        "docs = loader.load()\n",
        "print(len(docs), docs[0].metadata)"
      ],
      "metadata": {
        "id": "F43ZnwFXx5mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenizer**"
      ],
      "metadata": {
        "id": "xbTGfxD8jR_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# split into chunks\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "print(\"chunks:\", len(chunks))\n",
        "# each element in `chunks` is a Document with .page_content and .metadata\n",
        "print(\"first chunk content preview:\", chunks[0].page_content[:300])\n"
      ],
      "metadata": {
        "id": "SnNJ-VTKybGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Embedding**"
      ],
      "metadata": {
        "id": "NOTRThwRjYlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# Choose any HuggingFace embedding model\n",
        "emb = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")"
      ],
      "metadata": {
        "id": "U_1IdEBuzhgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **VectorDB and Retrieval**"
      ],
      "metadata": {
        "id": "sU35F6KdjrtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create FAISS index from your chunks\n",
        "index = FAISS.from_documents(chunks, emb)\n",
        "# Build retriever\n",
        "retriever = index.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "results_with_scores = index.similarity_search_with_score(\"A STRANGE TWIST OF FATE \", k=5)\n",
        "for doc, score in results_with_scores:\n",
        "    print(doc.metadata.get(\"source\"), score)\n"
      ],
      "metadata": {
        "id": "34PWbpcz0hlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gemini API Integration**"
      ],
      "metadata": {
        "id": "_eiRleY5kLmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = gkey\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",  # or gemini-1.5-pro\n",
        "    temperature=0,\n",
        ")\n",
        "prompt = \"Create a JSON query for: 'movies about aliens in 1980'\"\n",
        "\n",
        "# modern invoke\n",
        "response = llm.invoke(prompt)\n",
        "\n",
        "# --- Safe extraction of text content (new LCEL-compatible) ---\n",
        "if isinstance(response.content, str) and response.content.strip():\n",
        "    query_spec = response.content\n",
        "else:\n",
        "    # Gemini sometimes puts text inside response_metadata\n",
        "    query_spec = response.response_metadata[\"candidate_texts\"][0]\n",
        "\n",
        "print(query_spec)"
      ],
      "metadata": {
        "id": "4gWmojo21sGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LCEL Implementation**"
      ],
      "metadata": {
        "id": "7Xo32TqokTNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# minimal LCEL pipeline using index.similarity_search_with_score directly\n",
        "import itertools\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
        "\n",
        "# Runnable that calls the FAISS index directly and returns a mapping {context, question}\n",
        "search_runnable = RunnableLambda(\n",
        "    lambda inp: {\n",
        "        \"context\": \"\\n\\n\".join(\n",
        "            f\"[source:{(d.metadata.get('source') if getattr(d, 'metadata', None) else '')}] {d.page_content[:2000]}\"\n",
        "            for d, _score in index.similarity_search_with_score(inp[\"question\"], k=5)\n",
        "        ),\n",
        "        \"question\": inp[\"question\"],\n",
        "    }\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"You are a helpful assistant. Use ONLY the CONTEXT to answer the QUESTION.\n",
        "If the answer is not in the context, reply \"I don't know\".\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "Answer concisely.\"\"\"\n",
        ")\n",
        "\n",
        "pipeline = search_runnable | prompt | llm\n",
        "\n",
        "resp = pipeline.invoke({\"question\": \"Title of the chapters\"})\n",
        "\n",
        "# safe extraction\n",
        "out = getattr(resp, \"content\", None) or getattr(resp, \"text\", None) or str(resp)\n",
        "print(out.strip())\n"
      ],
      "metadata": {
        "id": "J3Dg7s6J3QRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RetrievalQA**"
      ],
      "metadata": {
        "id": "B1OVogehkkrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_classic.chains import RetrievalQA\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "# Gemini LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",    # or gemini-1.5-pro\n",
        "    temperature=0\n",
        ")\n",
        "# Create QA chains\n",
        "qa_stuff = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type='stuff',\n",
        "    retriever=retriever\n",
        ")\n",
        "# Query\n",
        "print(qa_stuff.run('Title of the chapters'))"
      ],
      "metadata": {
        "id": "I6sB5E-NYVy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_classic.chains import RetrievalQA\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "# Gemini LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",    # or gemini-1.5-pro\n",
        "    temperature=0\n",
        ")\n",
        "# Create QA chains\n",
        "qa_stuff = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type='map_reduce',\n",
        "    retriever=retriever\n",
        ")\n",
        "# Query\n",
        "print(qa_stuff.run('Title of the chapters'))"
      ],
      "metadata": {
        "id": "wfUDKt4uZHXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ConversationChain and ConversationBufferMemory**"
      ],
      "metadata": {
        "id": "7Ke4t6SoktNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_classic.chains import ConversationChain\n",
        "from langchain_classic.memory import ConversationBufferMemory\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
        "\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"history\", return_messages=True)\n",
        "\n",
        "chat = ConversationChain(llm=llm, memory=memory, verbose=False)\n",
        "\n",
        "print(\"Me   : Hi, my name is Shailesh.\")\n",
        "print(\"Bot  : \",chat.predict(input=\"Hi, my name is Shailesh.\"))\n",
        "print(\"Me   : Remember that I like sci-fi movies.\")\n",
        "print(\"Bot  : \"+chat.predict(input=\"Remember that I like sci-fi movies.\"))\n",
        "print(\"Me   : What is my name and what do I like?\")\n",
        "answer = chat.predict(input=\"What is my name and what do I like?\")\n",
        "print(\"Bot  : \"+answer)\n"
      ],
      "metadata": {
        "id": "hUIhburDYi-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tool Calling**"
      ],
      "metadata": {
        "id": "vEjrc5fhk2Hv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import tool\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "@tool\n",
        "def add(a: int, b: int):\n",
        "    \"\"\"Return a + b.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0,\n",
        "    model_kwargs={\n",
        "        \"tools\": [add]\n",
        "    }\n",
        ")\n",
        "\n",
        "response = llm.invoke(\"What is 12 + 9?\")\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "id": "RxzUHvrEZhFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Online RAG**"
      ],
      "metadata": {
        "id": "FDsvkhaqlM4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_classic.schema import BaseRetriever, Document   # BaseRetriever is the expected type\n",
        "from langchain_classic.chains import RetrievalQA\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetch_text(url: str) -> str:\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "    r = requests.get(url, headers=headers, timeout=10)\n",
        "    r.raise_for_status()\n",
        "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "    for s in soup([\"script\", \"style\"]):\n",
        "        s.decompose()\n",
        "    return \"\\n\".join(line.strip() for line in soup.get_text().splitlines() if line.strip())\n",
        "\n",
        "# create a Document from a live URL\n",
        "url = \"https://en.wikipedia.org/wiki/Artificial_intelligence\"\n",
        "page_text = fetch_text(url)\n",
        "web_doc = Document(page_content=page_text, metadata={\"source\": url})\n",
        "\n",
        "# Subclass BaseRetriever so RetrievalQA accepts it\n",
        "class WebOnlyRetriever(BaseRetriever):\n",
        "    # Declare the fields as class attributes with type hints.\n",
        "    # Pydantic will pick these up.\n",
        "    doc: Document\n",
        "    k: int = 4\n",
        "\n",
        "    # Removed the __init__ method. Pydantic's BaseModel will generate one\n",
        "    # that expects these fields as constructor arguments.\n",
        "\n",
        "    def _get_relevant_documents(self, query: str) -> list[Document]: # Implemented abstract method\n",
        "        # return the fetched document first (you can add ranking logic here)\n",
        "        return [self.doc]\n",
        "\n",
        "    async def _aget_relevant_documents(self, query: str) -> list[Document]: # Implemented abstract method\n",
        "        return self._get_relevant_documents(query)\n",
        "\n",
        "# instantiate retriever\n",
        "# Now, instantiate WebOnlyRetriever by passing the fields as keyword arguments\n",
        "combined = WebOnlyRetriever(doc=web_doc, k=4)\n",
        "\n",
        "# build LLM + classic RetrievalQA (legacy API)\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"map_reduce\", retriever=combined)\n",
        "\n",
        "print(qa.run(\"What is artificial Intelligence\"))"
      ],
      "metadata": {
        "id": "6SXw-1gZem2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Graph Rag**"
      ],
      "metadata": {
        "id": "6Z4Bfwofrq7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "def sparql(query):\n",
        "    url = \"https://query.wikidata.org/sparql\"\n",
        "    headers = {\"Accept\": \"application/sparql-results+json\"}\n",
        "    r = requests.get(url, params={\"query\": query}, headers=headers)\n",
        "    return r.json()\n",
        "\n",
        "entity = \"Q42\"   # Douglas Adams\n",
        "\n",
        "# retrieve graph neighbors\n",
        "q = f\"\"\"\n",
        "SELECT ?propLabel ?valueLabel WHERE {{\n",
        "  wd:{entity} ?prop ?value .\n",
        "  SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "}}\n",
        "LIMIT 30\n",
        "\"\"\"\n",
        "\n",
        "graph = sparql(q)\n",
        "print(graph)\n",
        "\n"
      ],
      "metadata": {
        "id": "ehxHLwJRrqpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# build simple context\n",
        "context = \"\\n\".join(\n",
        "    f\"{b['propLabel']['value']}: {b['valueLabel']['value']}\"\n",
        "    for b in graph[\"results\"][\"bindings\"]\n",
        ")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
        "\n",
        "prompt = f\"Use the following graph context to answer:\\n\\n{context}\\n\\nQ: Who is Douglas Adams?\"\n",
        "\n",
        "print(llm.invoke(prompt).content)"
      ],
      "metadata": {
        "id": "EP2CpKy4ruHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BVdvfotVt0qf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}